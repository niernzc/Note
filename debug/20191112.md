## KUBERNETES

* bug1

  ```shell
  [root@k8s-node1 ~]# kubectl get pods
  The connection to the server localhost:8080 was refused - did you specify the right host or port?
  ```

  问题原因： kubectl命令需要使用kubernetes-admin来运行，解决方法如下，将主节点中的【/etc/kubernetes/admin.conf】文件拷贝到从节点相同目录下，然后配置环境变量： 

  ```shell
  echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> ~/.bash_profile
  source ~/.bash_profile
  ```

  服务器之间文件传输可以使用scp：

  ```shell
  [root@k8s-master kubernetes]# scp admin.conf root@k8s-node1:/etc/kubernetes/
  ```

* 重启后需要重新通过`kubeadm init` 命令安装master

* pod起不来：

  ``` shell
  [root@k8s-master ~]# kubectl create -f mysql-rc.yaml 
  replicationcontroller/mysql created
  [root@k8s-master ~]# kubectl get rc
  NAME    DESIRED   CURRENT   READY   AGE
  mysql   1         1         0       8s
  [root@k8s-master ~]# kubectl get pod
  NAME          READY   STATUS    RESTARTS   AGE
  mysql-926tz   0/1     Pending   0          28s
  [root@k8s-node1 kubernetes]# kubectl describe pod mysql-926tz
  Name:           mysql-926tz
  Namespace:      default
  ···
    Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 2 node(s) had taints that the pod didn't tolerate.
  ```

  警告内容是：结点存在不能容忍的污点，解决办法：

  ```shell
  kubectl taint nodes --all node-role.kubernetes.io/master-
  ```

  ```shell
  [root@k8s-master ~]# kubectl taint nodes --all node-role.kubernetes.io/master-
  node/k8s-master untainted
  error: taint "node-role.kubernetes.io/master" not found
  [root@k8s-master ~]# kubectl get pod
  NAME          READY   STATUS    RESTARTS   AGE
  mysql-926tz   1/1     Running   0          9m4s
  ```

  ```shell
  [root@k8s-master ~]# docker ps | grep mysql
  a976d044ea3a        mysql                                                           "docker-entrypoint.s…"   2 minutes ago       Up 2 minutes                            k8s_mysql_mysql-926tz_default_b09891a0-94fe-47e9-826a-12e7615b0498_0
  812ff5a5b4ca        registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1   "/pause"                 2 minutes ago       Up 2 minutes                            k8s_POD_mysql-926tz_default_b09891a0-94fe-47e9-826a-12e7615b0498_0
  ```

* ```shell
  [root@k8s-master ~]# kubectl create -f myweb-rc.yaml 
  error: error parsing myweb-rc.yaml: error converting YAML to JSON: yaml: line 22: found character that cannot start any token
  ```

  ***yaml不能有制表符***

## Kubernetes 安装（最少配置）

1. 配置yum源：

   ```shell
   [root@localhost ~]# cd /etc/yum.repos.d/
   [root@localhost ~]# vim /etc/yum.repos.d/kubernetes.repo 
   
   [kubernetes]
   name=Kubernetes Repository
   baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
   enabled=1
   gpgcheck=0
   ```

2. 安装Kubectl、kubeadm、kubelet

   ```shell
   [root@localhost yum.repos.d]# yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
   [root@localhost yum.repos.d]# systemctl enable docker && systemctl start docker
   Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.
   [root@localhost yum.repos.d]# systemctl enable kubelet && systemctl start kubelet
   Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service.
   ```

3. 下载相关镜像：

   ```shell
   [root@localhost ~]# mkdir kubernetes-learning
   [root@localhost ~]# cd kubernetes-learning/
   [root@localhost kubernetes-learning]# mkdir start
   [root@localhost kubernetes-learning]# cd start/
   
   [root@localhost start]# vim init-config.yaml 
   apiVersion: kubeadm.k8s.io/v1beta1
   kind: ClusterConfiguration
   imageRepository:  registry.cn-hangzhou.aliyuncs.com/google_containers
   kubernetesVersion: v1.16.0
   networking:
     podSubnet: "192.168.0.0/16"
   
   [root@localhost start]# kubeadm config images pull --config=init-config.yaml 
   [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.16.0
   [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.16.0
   [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.16.0
   [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.16.0
   [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1
   [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.3.15-0
   [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.6.2
   ```

4. 安装master：

   ```shell
   [root@localhost start]# kubeadm init --config=init-config.yaml 
   [init] Using Kubernetes version: v1.16.0
   [preflight] Running pre-flight checks
   	[WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
   error execution phase preflight: [preflight] Some fatal errors occurred:
   	[ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1
   	[ERROR Swap]: running with swap on is not supported. Please disable swap
   ```

   解决办法：

   ```shell
   [root@localhost start]# echo 1 > /proc/sys/net/bridge/bridge-nf-call-iptables
   [root@localhost start]# echo 1 > /proc/sys/net/bridge/bridge-nf-call-ip6tables
   [root@localhost start]# sed -i "s/\/dev\/mapper\/centos-swap/\#\/dev\/mapper\/centos-swap/g" /etc/fstab
   ```

   ```shell
   [root@localhost start]# kubeadm init --config=init-config.yaml 
   [init] Using Kubernetes version: v1.16.0
   [preflight] Running pre-flight checks
   	[WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
   error execution phase preflight: [preflight] Some fatal errors occurred:
   	[ERROR Swap]: running with swap on is not supported. Please disable swap
   ```

   swap的问题没有解决

   ```shell
   [root@localhost start]# systemctl daemon-reload
   [root@localhost start]# systemctl restart kubelet
   [root@localhost start]# kubeadm init --config=init-config.yaml 
   [init] Using Kubernetes version: v1.16.0
   [preflight] Running pre-flight checks
   	[WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
   error execution phase preflight: [preflight] Some fatal errors occurred:
   	[ERROR Swap]: running with swap on is not supported. Please disable swap
   ```

   ```shell
   [root@localhost start]# swapoff -a
   [root@localhost start]# kubeadm init --config=init-config.yaml 
   [init] Using Kubernetes version: v1.16.0
   [preflight] Running pre-flight checks
   	[WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
   [preflight] Pulling images required for setting up a Kubernetes cluster
   [preflight] This might take a minute or two, depending on the speed of your internet connection
   [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
   ···
   [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
   [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
   [kubelet-check] Initial timeout of 40s passed.
   
   Unfortunately, an error has occurred:
   	timed out waiting for the condition
   
   This error is likely caused by:
   	- The kubelet is not running
   	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)
   
   If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
   	- 'systemctl status kubelet'
   	- 'journalctl -xeu kubelet'
   
   Additionally, a control plane component may have crashed or exited when started by the container runtime.
   To troubleshoot, list all containers using your preferred container runtimes CLI, e.g. docker.
   Here is one example how you may list all Kubernetes containers running in docker:
   	- 'docker ps -a | grep kube | grep -v pause'
   	Once you have found the failing container, you can inspect its logs with:
   	- 'docker logs CONTAINERID'
   error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
   To see the stack trace of this error execute with --v=5 or higher
   
   ```

   检查kubelet：

   ```shell
   [root@localhost ~]# systemctl status kubelet
   ● kubelet.service - kubelet: The Kubernetes Node Agent
      Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)
     Drop-In: /usr/lib/systemd/system/kubelet.service.d
              └─10-kubeadm.conf
      Active: active (running) since Tue 2019-11-12 06:14:33 EST; 8min ago
        Docs: https://kubernetes.io/docs/
    Main PID: 3754 (kubelet)
      CGroup: /system.slice/kubelet.service
              └─3754 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/boot...
   
   Nov 12 06:22:53 localhost.localdomain kubelet[3754]: E1112 06:22:53.322994    ...
   Nov 12 06:22:53 localhost.localdomain kubelet[3754]: E1112 06:22:53.423556    ...
   Nov 12 06:22:53 localhost.localdomain kubelet[3754]: E1112 06:22:53.459015    ...
   Nov 12 06:22:53 localhost.localdomain kubelet[3754]: E1112 06:22:53.524170    ...
   Nov 12 06:22:53 localhost.localdomain kubelet[3754]: E1112 06:22:53.624753    ...
   Nov 12 06:22:53 localhost.localdomain kubelet[3754]: E1112 06:22:53.659857    ...
   Nov 12 06:22:53 localhost.localdomain kubelet[3754]: E1112 06:22:53.725854    ...
   Nov 12 06:22:53 localhost.localdomain kubelet[3754]: W1112 06:22:53.736745    ...
   Nov 12 06:22:53 localhost.localdomain kubelet[3754]: E1112 06:22:53.826564    ...
   Nov 12 06:22:53 localhost.localdomain kubelet[3754]: E1112 06:22:53.859872    ...
   Hint: Some lines were ellipsized, use -l to show in full.
   ```

   状态是**active**

   重试：

   ```shell
   [root@localhost start]# kubeadm init --config=init-config.yaml 
   [init] Using Kubernetes version: v1.16.0
   [preflight] Running pre-flight checks
   	[WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correc
   error execution phase preflight: [preflight] Some fatal errors occurred:
   	[ERROR Port-10251]: Port 10251 is in use
   	[ERROR FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml]: /etc/kubernetes/manifests/kube-apiserver.yaml already 
   	[ERROR FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml]: /etc/kubernetes/manifests/kube-controller-man
   	[ERROR FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml]: /etc/kubernetes/manifests/kube-scheduler.yaml already 
   	[ERROR FileAvailable--etc-kubernetes-manifests-etcd.yaml]: /etc/kubernetes/manifests/etcd.yaml already exists
   	[ERROR Port-10250]: Port 10250 is in use
   [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
   To see the stack trace of this error execute with --v=5 or higher
   ```

   陪烂了:

   ```shell
   [root@localhost start]# kubeadm reset
   [root@localhost start]# kubeadm init --config=init-config.yaml 
   [init] Using Kubernetes version: v1.16.0
   [preflight] Running pre-flight checks
   	[WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correc
   [preflight] Pulling images required for setting up a Kubernetes cluster
   [preflight] This might take a minute or two, depending on the speed of your internet connection
   [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
   [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
   [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
   ···
   [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
   [kubelet-check] Initial timeout of 40s passed.
   ^C
   ```

   `kubeadm reset` 真是个好东西

   先解决前面防火墙的问题：

   ```shell
   [root@localhost start]# setenforce 0
   [root@localhost start]# cat -n /etc/selinux/config 
        1	
        2	# This file controls the state of SELinux on the system.
        3	# SELINUX= can take one of these three values:
        4	#     enforcing - SELinux security policy is enforced.
        5	#     permissive - SELinux prints warnings instead of enforcing.
        6	#     disabled - No SELinux policy is loaded.
        7	SELINUX=enforcing
        8	# SELINUXTYPE= can take one of three values:
        9	#     targeted - Targeted processes are protected,
       10	#     minimum - Modification of targeted policy. Only selected processes are protected. 
       11	#     mls - Multi Level Security protection.
       12	SELINUXTYPE=targeted 
       13	
       14	
   [root@localhost start]# sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
   ```

   <font color=red>根据文档、教程修改系统文件之前应该先看看文件内容，以及修改内容，最好留一个备份先</font>

   ```shell 
   [root@localhost start]# kubeadm init --config=init-config.yaml 
   [init] Using Kubernetes version: v1.16.0
   [preflight] Running pre-flight checks
   	[WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correc
   [preflight] Pulling images required for setting up a Kubernetes cluster
   [preflight] This might take a minute or two, depending on the speed of your internet connection
   [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
   [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
   [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
   [kubelet-start] Activating the kubelet service
   [certs] Using certificateDir folder "/etc/kubernetes/pki"
   [certs] Generating "ca" certificate and key
   [certs] Generating "apiserver" certificate and key
   [certs] apiserver serving cert is signed for DNS names [localhost.localdomain kubernetes kubernetes.default kubernetes.default.svc ter.local] and IPs [10.96.0.1 192.168.182.133]
   [certs] Generating "apiserver-kubelet-client" certificate and key
   [certs] Generating "front-proxy-ca" certificate and key
   [certs] Generating "front-proxy-client" certificate and key
   [certs] Generating "etcd/ca" certificate and key
   [certs] Generating "etcd/server" certificate and key
   [certs] etcd/server serving cert is signed for DNS names [localhost.localdomain localhost] and IPs [192.168.182.133 127.0.0.1 ::1]
   [certs] Generating "etcd/peer" certificate and key
   [certs] etcd/peer serving cert is signed for DNS names [localhost.localdomain localhost] and IPs [192.168.182.133 127.0.0.1 ::1]
   [certs] Generating "etcd/healthcheck-client" certificate and key
   [certs] Generating "apiserver-etcd-client" certificate and key
   [certs] Generating "sa" key and public key
   [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
   [kubeconfig] Writing "admin.conf" kubeconfig file
   [kubeconfig] Writing "kubelet.conf" kubeconfig file
   [kubeconfig] Writing "controller-manager.conf" kubeconfig file
   [kubeconfig] Writing "scheduler.conf" kubeconfig file
   [control-plane] Using manifest folder "/etc/kubernetes/manifests"
   [control-plane] Creating static Pod manifest for "kube-apiserver"
   [control-plane] Creating static Pod manifest for "kube-controller-manager"
   [control-plane] Creating static Pod manifest for "kube-scheduler"
   [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
   [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
   [apiclient] All control plane components are healthy after 17.003242 seconds
   [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
   [kubelet] Creating a ConfigMap "kubelet-config-1.16" in namespace kube-system with the configuration for the kubelets in the cluste
   [upload-certs] Skipping phase. Please see --upload-certs
   [mark-control-plane] Marking the node localhost.localdomain as control-plane by adding the label "node-role.kubernetes.io/master=''
   [mark-control-plane] Marking the node localhost.localdomain as control-plane by adding the taints [node-role.kubernetes.io/master:N
   [bootstrap-token] Using token: lsc5xq.yrs3qn212cind7i6
   [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
   [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate
   [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
   [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
   [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
   [addons] Applied essential addon: CoreDNS
   [addons] Applied essential addon: kube-proxy
   
   Your Kubernetes control-plane has initialized successfully!
   
   To start using your cluster, you need to run the following as a regular user:
   
     mkdir -p $HOME/.kube
     sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
     sudo chown $(id -u):$(id -g) $HOME/.kube/config
   
   You should now deploy a pod network to the cluster.
   Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
     https://kubernetes.io/docs/concepts/cluster-administration/addons/
   
   Then you can join any number of worker nodes by running the following on each as root:
   
   kubeadm join 192.168.182.133:6443 --token lsc5xq.yrs3qn212cind7i6 \
       --discovery-token-ca-cert-hash sha256:7681a1d96748cffcd787234553f116be3c44df918782eb88df30681a44b09ad2 
   ```

   安装完成

## 部署第一个应用

```shell
[root@localhost kubernetes-learning]# mkdir helloworld
[root@localhost kubernetes-learning]# cd helloworld/
[root@localhost helloworld]# 
```

```shell
[root@localhost helloworld]# vim mysql-rc.yaml
apiVersion: v1
kind: ReplicationController
metadata: 
  name: mysql
spec:
  replicas: 1
  selector: 
    app: mysql
  template:
    metadata: 
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql
        ports:
        - containerPort: 3306
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: "123456"
```

```shell
[root@localhost helloworld]# kubectl create -f mysql-rc.yaml 
The connection to the server localhost:8080 was refused - did you specify the right host or port?
#检查hosts，但没有问题
[root@localhost helloworld]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
#发现有一步忘了做：
[root@localhost helloworld]# cd ~
[root@localhost ~]# mkdir -p .kube
[root@localhost ~]# cp -i /etc/kubernetes/admin.conf .kube/config
[root@localhost .kube]# cd ~/kubernetes-learning/helloworld/
[root@localhost helloworld]# kubectl create -f mysql-rc.yaml 
replicationcontroller/mysql created
#rc创建成功，检查pod状态：
[root@localhost helloworld]# kubectl get pod
NAME          READY   STATUS    RESTARTS   AGE
mysql-x8q24   0/1     Pending   0          13m
#trouble-shoot
[root@localhost helloworld]# kubectl describe pod mysql-x8q24
···
Events:
  Type     Reason            Age        From               Message
  ----     ------            ----       ----               -------
  Warning  FailedScheduling  <unknown>  default-scheduler  0/1 nodes are available: 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/1 nodes are available: 1 node(s) had taints that the pod didn't tolerate.
```

