新建两个虚拟机`misaka-centos-131`、`misaka-centos-132`

以131作为master

* 修改主机名称并设置hostname解析

  ```shell
  [root@localhost ~]# hostnamectl set-hostname k8s-master
  [root@localhost ~]# echo "127.0.0.1   $(hostname)" >> /etc/hosts
  [root@localhost ~]# sh
  sh-4.2# bash
  [root@k8s-master ~]# 
  ```

* 安装docker nfs-utils kubectl kubeadm kubelet

  ```shell
  [root@k8s-master ~]# curl -sSL https://kuboard.cn/install-script/v1.16.0/install-kubelet.sh | sh
  ```

* 初始化master结点

  ```shell
  [root@k8s-master ~]# export MASTER_IP=192.168.182.131
  [root@k8s-master ~]# export APISERVER_NAME=apiserver.demo
  [root@k8s-master ~]# export POD_SUBNET=10.100.0.1/20
  [root@k8s-master ~]# echo "${MASTER_IP}    ${APISERVER_NAME}" >> /etc/hosts
  [root@k8s-master ~]# curl -sSL https://kuboard.cn/install-script/v1.16.0/init-master.sh | sh
  
  [init] Using Kubernetes version: v1.16.0
  [preflight] Running pre-flight checks
  [preflight] Pulling images required for setting up a Kubernetes cluster
  [preflight] This might take a minute or two, depending on the speed of your internet connection
  [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
  [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
  [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
  [kubelet-start] Activating the kubelet service
  [certs] Using certificateDir folder "/etc/kubernetes/pki"
  [certs] Generating "ca" certificate and key
  [certs] Generating "apiserver" certificate and key
  [certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local apiserver.demo] and IPs [10.96.0.1 192.168.182.131]
  [certs] Generating "apiserver-kubelet-client" certificate and key
  [certs] Generating "front-proxy-ca" certificate and key
  [certs] Generating "front-proxy-client" certificate and key
  [certs] Generating "etcd/ca" certificate and key
  [certs] Generating "etcd/server" certificate and key
  [certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.182.131 127.0.0.1 ::1]
  [certs] Generating "etcd/peer" certificate and key
  [certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.182.131 127.0.0.1 ::1]
  [certs] Generating "etcd/healthcheck-client" certificate and key
  [certs] Generating "apiserver-etcd-client" certificate and key
  [certs] Generating "sa" key and public key
  [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
  [kubeconfig] Writing "admin.conf" kubeconfig file
  [kubeconfig] Writing "kubelet.conf" kubeconfig file
  [kubeconfig] Writing "controller-manager.conf" kubeconfig file
  [kubeconfig] Writing "scheduler.conf" kubeconfig file
  [control-plane] Using manifest folder "/etc/kubernetes/manifests"
  [control-plane] Creating static Pod manifest for "kube-apiserver"
  [control-plane] Creating static Pod manifest for "kube-controller-manager"
  [control-plane] Creating static Pod manifest for "kube-scheduler"
  [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
  [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
  [apiclient] All control plane components are healthy after 16.001877 seconds
  [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
  [kubelet] Creating a ConfigMap "kubelet-config-1.16" in namespace kube-system with the configuration for the kubelets in the cluster
  [upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
  [upload-certs] Using certificate key:
  7d0390b78191e4d8989b49e41a62de53783e167bc83c03ad58597c383a76db40
  [mark-control-plane] Marking the node k8s-master as control-plane by adding the label "node-role.kubernetes.io/master=''"
  [mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
  [bootstrap-token] Using token: sdem7l.49eba2sw7fxy49i5
  [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
  [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
  [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
  [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
  [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
  [addons] Applied essential addon: CoreDNS
  [addons] Applied essential addon: kube-proxy
  
  Your Kubernetes control-plane has initialized successfully!
  
  To start using your cluster, you need to run the following as a regular user:
  
    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config
  
  You should now deploy a pod network to the cluster.
  Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
    https://kubernetes.io/docs/concepts/cluster-administration/addons/
  
  You can now join any number of the control-plane node running the following command on each as root:
  
    kubeadm join apiserver.demo:6443 --token sdem7l.49eba2sw7fxy49i5 \
      --discovery-token-ca-cert-hash sha256:15d02e14e29e10cf2c1b3a78981747c7867da57bce96a380691cc101a815197d \
      --control-plane --certificate-key 7d0390b78191e4d8989b49e41a62de53783e167bc83c03ad58597c383a76db40
  
  Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
  As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use 
  "kubeadm init phase upload-certs --upload-certs" to reload certs afterward.
  
  Then you can join any number of worker nodes by running the following on each as root:
  
  kubeadm join apiserver.demo:6443 --token sdem7l.49eba2sw7fxy49i5 \
      --discovery-token-ca-cert-hash sha256:15d02e14e29e10cf2c1b3a78981747c7867da57bce96a380691cc101a815197d 
  sh: line 31: wget: command not found
  sed: can't read calico.yaml: No such file or directory
  error: the path "calico.yaml" does not exist
  请确保您正在使用 https://kuboard.cn/install/install-k8s.html 上的最新K8S安装文档，并加入了在线答疑QQ群，以避免碰到问题时无人解答
  
  ```

  比较[之前]( D:\sysu\note\starLight\k8s\安装k8s.md )的源码所做的事情：初始化master、配置kubectl、安装calico网络插件：这里只有网络插件没有装好，但kubernetes已经装好了

